{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /mnt/Woo/text-generation-webui/models/TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from string import Template\n",
    "\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    "    ExLlamaV2Lora,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2StreamingGenerator,\n",
    "    ExLlamaV2Sampler,\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize model and cache\n",
    "\n",
    "model_directory = (\n",
    "    \"/mnt/Woo/text-generation-webui/models/TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
    ")\n",
    "config = ExLlamaV2Config()\n",
    "config.model_dir = model_directory\n",
    "config.prepare()\n",
    "\n",
    "model = ExLlamaV2(config)\n",
    "print(\"Loading model: \" + model_directory)\n",
    "model.load()\n",
    "\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "cache = ExLlamaV2Cache(model)\n",
    "\n",
    "# Load LoRA\n",
    "\n",
    "lora_directory = \"/mnt/Woo/text-generation-webui/loras/Wizard-Vicuna-13B-Uncensored-GPTQ-reddit-submissions\"\n",
    "lora = ExLlamaV2Lora.from_directory(model, lora_directory)\n",
    "\n",
    "# Initialize generators\n",
    "\n",
    "streaming_generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
    "streaming_generator.warmup()\n",
    "\n",
    "streaming_generator.set_stop_conditions([\"\\nUser:\", tokenizer.eos_token_id])\n",
    "\n",
    "simple_generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Sampling settings\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.token_repetition_penalty = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_lora(prompt_, lora_, max_new_tokens):\n",
    "    print(prompt_, end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt_)\n",
    "\n",
    "    streaming_generator.begin_stream(input_ids, settings, loras=lora_)\n",
    "    generated_tokens = 0\n",
    "    output = \"\"\n",
    "    while True:\n",
    "        chunk, eos, _ = streaming_generator.stream()\n",
    "        generated_tokens += 1\n",
    "        output += chunk\n",
    "        print(chunk, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        if eos or generated_tokens == max_new_tokens:\n",
    "            break\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"done\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "POST_TEMPLATE = Template(\n",
    "    \"\"\"You are a Reddit post generator.\n",
    "User: \n",
    "Subreddit: $subreddit \n",
    "Author: $author \n",
    "Media: $media \n",
    "Title: $title \n",
    "Write the Reddit post.\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "TAGS = {\n",
    "    \"Subreddit\": \"[SUBREDDIT]\",\n",
    "    \"Author\": \"[AUTHOR]\",\n",
    "    \"Media\": \"[MEDIA]\",\n",
    "    \"Title\": \"[TITLE]\",\n",
    "    \"EOS\": \"\"\"Write the Reddit post.\n",
    "Assistant:\"\"\",\n",
    "}\n",
    "\n",
    "POST_TAGS_ORDER_OP = [\n",
    "    TAGS[\"Subreddit\"],\n",
    "    TAGS[\"Author\"],\n",
    "    TAGS[\"Media\"],\n",
    "    TAGS[\"Title\"],\n",
    "    TAGS[\"EOS\"],\n",
    "]  # order matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a Reddit post generator.\n",
      "User: \n",
      "Subreddit: r/aww \n",
      "Author: u/aww \n",
      "Media: https://i.redd.it/1q2w3e4r5t6y.jpg \n",
      "Title: \n",
      "generate from here to: '\\n'\n",
      "\n",
      "You are a Reddit post generator.\n",
      "User: \n",
      "Subreddit: r/aww \n",
      "Author: u/aww \n",
      "Media: https://i.redd.it/1q2w3e4r5t6y.jpg \n",
      "Title: 19 years ago today, I was born to be your mom. \n",
      "done\n",
      "generated output: 19 years ago today, I was born to be your mom. \n"
     ]
    }
   ],
   "source": [
    "def get_up_to_tag_line(prompt, tag):\n",
    "    try:\n",
    "        sub_index = prompt.index(tag)\n",
    "    except ValueError:\n",
    "        print(\"Error: tag not found\")\n",
    "        return -1\n",
    "\n",
    "    # get last \\n before tag\n",
    "    last_newline_index = prompt.rfind(\"\\n\", 0, sub_index)\n",
    "\n",
    "    if last_newline_index == -1:\n",
    "        last_newline_index = 0\n",
    "    # remove  from last_newline_index to newline_index\n",
    "    prompt = prompt[:last_newline_index]\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_up_to_tag(prompt, tag):\n",
    "    try:\n",
    "        sub_index = prompt.index(tag)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "    prompt = prompt[:sub_index]\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_post(\n",
    "    subreddit=TAGS[\"Subreddit\"],\n",
    "    author=TAGS[\"Author\"],\n",
    "    media=TAGS[\"Media\"],\n",
    "    title=TAGS[\"Title\"],\n",
    "):\n",
    "    prompt_full = POST_TEMPLATE.substitute(\n",
    "        subreddit=subreddit, author=author, media=media, title=title\n",
    "    )\n",
    "\n",
    "    # loop over tags in order\n",
    "    for tag in POST_TAGS_ORDER_OP:\n",
    "        if tag == TAGS[\"EOS\"]:\n",
    "            break\n",
    "        prompt = get_up_to_tag(prompt_full, tag)\n",
    "        if prompt == -1:\n",
    "            continue  # skip tag if not found\n",
    "        print(prompt)\n",
    "\n",
    "        # get tag key from tag value\n",
    "        tag_key = list(TAGS.keys())[list(TAGS.values()).index(tag)]\n",
    "        next_tag_value = POST_TAGS_ORDER_OP[POST_TAGS_ORDER_OP.index(tag) + 1]\n",
    "        next_tag_key = list(TAGS.keys())[list(TAGS.values()).index(next_tag_value)]\n",
    "        # print(\"Tag: \" + tag_key)\n",
    "        print(f\"\"\"generate from here to: '\\\\n'\"\"\")\n",
    "        print()\n",
    "        streaming_generator.set_stop_conditions(\n",
    "            [\"\\n\", \"\\nUser:\", tokenizer.eos_token_id]\n",
    "        )\n",
    "        output = generate_with_lora(prompt, lora, 100)\n",
    "        print(f\"generated output: {output}\")\n",
    "\n",
    "    # print(prompt_full)\n",
    "\n",
    "\n",
    "# generate_post()\n",
    "print()\n",
    "# generate_post(subreddit=\"r/aww\", author=\"u/aww\", media=\"https://i.redd.it/1q2w3e4r5t6y.jpg\", title=\"Cute doggo\")\n",
    "generate_post(\n",
    "    subreddit=\"r/aww\", author=\"u/aww\", media=\"https://i.redd.it/1q2w3e4r5t6y.jpg\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
